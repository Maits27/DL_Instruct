{
  "do_train": true,
  "do_eval": true,
  "do_predict": false,
  "train_file": "MI_PATH/evaluador/data/casimedicos/CARPETA_NOMBRE/VAR_LANG_train_casimedicos.json",
  "dev_file": "MI_PATH/evaluador/data/casimedicos/CARPETA_NOMBRE/VAR_LANG_dev_casimedicos.json",
  "test_file": "MI_PATH/evaluador/data/casimedicos/CARPETA_NOMBRE/VAR_LANG_test_casimedicos.json",
  "output_dir": "MI_PATH/evaluador/train_src/outputQA/CARPETA_NOMBRE/MODEL_ID_NAME/output_VAR_LR_VAR_WD_VAR_WARM",
  "overwrite_output_dir": false,
  "overwrite_cache": true,
  "model_path": "MODEL_ID",
  "language": "VAR_LANG",
  "task_name": "casimedicos",
  "seed": 42,
  "num_train_epochs": 10,
  "per_device_train_batch_size": 4,
  "per_device_eval_batch_size": 4,
  "gradient_accumulation_steps": 8,
  "learning_rate": VAR_LR,
  "weight_decay": VAR_WD,
  "warmup_ratio": VAR_WARM,
  "metric_for_best_model": "eval_accuracy",
  "evaluation_strategy": "epoch",
  "save_strategy": "epoch",
  "load_best_model_at_end": true,
  "loading_script_path": "MI_PATH/evaluador/train_src/scripts/load.py",
  "config_name": null,
  "tokenizer_name": null,
  "cache_dir": null,
  "run_name": "Train MODEL_ID_NAME VAR_LR VAR_WD VAR_WARM",
  "report_to": "wandb",
  "modules": {
    "meta-llama/Llama-3.1-8B": ["self_attn.q_proj",
    "self_attn.k_proj",
    "self_attn.v_proj",
    "self_attn.o_proj",
    "mlp.gate_proj",
    "mlp.up_proj",
    "mlp.down_proj"
    ],
    "meta-llama/Llama-3.1-70B": ["self_attn.q_proj",
    "self_attn.k_proj",
    "self_attn.v_proj",
    "self_attn.o_proj",
    "mlp.gate_proj",
    "mlp.up_proj",
    "mlp.down_proj"
    ],
    "google/gemma-2-9b": ["self_attn.q_proj",
    "self_attn.k_proj",
    "self_attn.v_proj",
    "self_attn.o_proj",
    "mlp.gate_proj",
    "mlp.up_proj",
    "mlp.down_proj"
    ],
    "mistralai/Mistral-7B-v0.3": ["self_attn.q_proj",
    "self_attn.k_proj",
    "self_attn.v_proj",
    "self_attn.o_proj",
    "mlp.gate_proj",
    "mlp.up_proj",
    "mlp.down_proj"
    ]
  }
}
